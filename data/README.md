# Tritter Training Data

This directory contains curated training datasets for the Tritter multimodal transformer.

## Philosophy

**Quality over quantity.** Following the guidance in `docs/TRAINING_STRATEGY.md`, we prioritize:

- Permissive licensing (MIT, Apache-2.0, BSD variants)
- High-quality repositories (>100 stars, well-maintained)
- Aggressive deduplication (exact hash + near-duplicate filtering)
- Domain-specific filtering (remove auto-generated code, minified files)

## Directory Structure

```
data/
├── README.md                    # This file
├── curated_repos.json          # High-quality repository lists
├── curated/                    # Filtered datasets ready for training
│   ├── python_curated.jsonl
│   ├── rust_curated.jsonl
│   └── javascript_curated.jsonl
├── raw/                        # Unprocessed downloads (gitignored)
└── processed/                  # Intermediate processing stages (gitignored)
```

## Running Dataset Curation

### List Available Datasets

```bash
python scripts/curate_datasets.py --list-subsets
```

Output:
```
Available Stack v2 subsets:
  python:
    Size: 233 GB
    Files: 57,000,000
    Description: Python code from Software Heritage
  rust:
    Size: 15.6 GB
    Files: 2,200,000
    Description: Rust code from Software Heritage
  ...
```

### Download and Filter Python Code

```bash
# Full dataset (will take hours/days)
python scripts/curate_datasets.py --language python --output data/curated

# Limited sample for testing (1000 samples)
python scripts/curate_datasets.py --language python --max-samples 1000 --output data/curated

# High-quality filter (min 500 stars)
python scripts/curate_datasets.py --language python --min-stars 500 --output data/curated
```

### Download Rust Code

```bash
python scripts/curate_datasets.py --language rust --min-stars 100 --output data/curated
```

### Dry Run (Count Without Saving)

```bash
python scripts/curate_datasets.py --language python --min-stars 100 --dry-run
```

## Output Format

All curated datasets are saved as JSONL (JSON Lines) with the following schema:

```json
{
  "text": "def hello():\n    print('world')",
  "language": "python",
  "license": "mit",
  "stars": 1500,
  "path": "src/main.py",
  "repo": "owner/repository"
}
```

### Fields

| Field | Type | Description |
|-------|------|-------------|
| `text` | string | Full code content |
| `language` | string | Programming language (python, rust, javascript) |
| `license` | string | SPDX license identifier (lowercase) |
| `stars` | integer | Maximum GitHub stars for the repository |
| `path` | string | Relative file path within repository |
| `repo` | string | GitHub repository identifier |

## Quality Criteria

From `TRAINING_STRATEGY.md`, we apply these filters:

### License Filtering

Only permissive licenses are included:
- `mit`
- `apache-2.0`
- `bsd-2-clause`
- `bsd-3-clause`
- `isc`
- `unlicense`
- `cc0-1.0`

### Repository Quality

- Minimum GitHub stars: 100 (default, configurable)
- Maintained repositories (implicit via Stack v2 curation)
- Well-documented (presence of README, docstrings)

### File Quality

**Include:**
- 10-10,000 lines (filters trivial and auto-generated files)
- <500KB file size (prevents minified/bundled code)
- >25% alphabetic character ratio (filters binary/encoded data)
- <10% lines over 500 characters (filters minified code)

**Exclude:**
- Auto-generated markers: "auto-generated", "do not edit", "generated by"
- Configuration files (handled by Stack v2 extension filtering)
- Minified/obfuscated code
- Binary or encoded data

### Deduplication

1. **Exact deduplication**: MD5 hash-based removal (in-memory during processing)
2. **Near-duplicate**: Planned post-processing with MinHash (Jaccard 0.7-0.8)
3. **Benchmark contamination**: Post-processing removal of HumanEval, MBPP, DS-1000

## Data Sources

### Primary: The Stack v2 (BigCode)

- **HuggingFace Path**: `bigcode/the-stack-v2-dedup`
- **License**: Per-file permissive licensing
- **Size**: 67TB total (233GB Python, 15.6GB Rust)
- **Quality**: Near-deduplicated, PII removed
- **Access**: Requires HuggingFace datasets library

```bash
pip install datasets
```

### Secondary: Stack-Edu (Educational Quality)

For highest-quality subsets, consider Stack-Edu:

```python
from datasets import load_dataset

# Python educational quality subset
ds = load_dataset("HuggingFaceTB/stack-edu", data_dir="python")

# Rust educational quality subset
ds = load_dataset("HuggingFaceTB/stack-edu", data_dir="rust")
```

### Curated Repository Lists

See `curated_repos.json` for manually selected high-quality repositories.

## Training Data Mix

From `TRAINING_STRATEGY.md`, Phase 1 continued pretraining targets:

| Component | Percentage | Tokens | Source |
|-----------|------------|--------|--------|
| Python Code | 40% | 40B | Stack v2 Python (this pipeline) |
| Rust Code | 25% | 25B | Stack v2 Rust (this pipeline) |
| High-quality repos | 20% | 20B | Curated repos list |
| Technical docs | 10% | 10B | Framework docs (manual scraping) |
| Persona conversations | 5% | 5B | Synthetic (Constitutional AI) |

Total: ~100B tokens for Phase 1

## Expected Data Retention Rates

Based on StarCoder2 and DeepSeek-Coder pipelines:

| Filter Stage | Retention Rate |
|--------------|----------------|
| License filtering | 30-50% |
| Basic filters (lines, size) | 80-90% |
| Exact deduplication | 90-95% |
| Quality filters (alpha ratio, long lines) | 70-85% |
| **Overall** | **15-30%** |

For Python (57M files), expect **8.5-17M filtered files**.

For Rust (2.2M files), expect **330K-660K filtered files**.

## Next Steps

1. **Run curation for Python**:
   ```bash
   python scripts/curate_datasets.py --language python --output data/curated
   ```

2. **Run curation for Rust**:
   ```bash
   python scripts/curate_datasets.py --language rust --output data/curated
   ```

3. **Verify output**:
   ```bash
   wc -l data/curated/python_curated.jsonl
   head -n 1 data/curated/python_curated.jsonl | jq .
   ```

4. **Near-duplicate filtering** (post-processing):
   ```bash
   # TODO: Implement MinHash LSH deduplication
   # Expected: 55-70% retention after near-dedup
   ```

5. **Benchmark decontamination** (post-processing):
   ```bash
   # TODO: Remove HumanEval, MBPP, DS-1000 samples
   ```

## Storage Requirements

| Dataset | Raw Size | Filtered Size (est.) |
|---------|----------|----------------------|
| Python | 233 GB | 35-70 GB (15-30%) |
| Rust | 15.6 GB | 2.3-4.7 GB (15-30%) |
| JavaScript | 87 GB | 13-26 GB (15-30%) |

Ensure sufficient disk space before starting curation.

## Integration with Training

Once curated, datasets can be loaded for training:

```python
from datasets import load_dataset

# Load curated Python dataset
ds = load_dataset("json", data_files="data/curated/python_curated.jsonl")

# Streaming mode for large datasets
ds = load_dataset("json", data_files="data/curated/python_curated.jsonl", streaming=True)
```

## References

- [TRAINING_STRATEGY.md](../docs/TRAINING_STRATEGY.md) - Overall training strategy
- [clean-datasets.md](../docs/clean-datasets.md) - Dataset sources and quality criteria
- [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2) - Primary data source
- [Stack-Edu](https://huggingface.co/datasets/HuggingFaceTB/stack-edu) - Educational quality subset

---

*Last Updated: 2026-01-23*
